{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e50e724d-04b7-4bd9-9515-e3a0d3fcd781",
   "metadata": {},
   "source": [
    "# Embedding Texts "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd300281-21c2-4f2f-bd2c-dd8453ed9aea",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "616ce10a-2b99-4972-b3a9-0c3477afa7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f1de9db8-b70d-4c06-ab5b-9118bfbab769",
   "metadata": {},
   "outputs": [],
   "source": [
    "CAPTIONS_FILE = '/Users/harshakhiyani/My_Stuff/CODE/GITHUB/Visual-Situmlai-Reconstruction-Using-fMRI-and-Deep-Learning/NSD-Flat Data Proccessing/Caption 77 tokens/sample_subject_0/captions/captions.txt'\n",
    "SAVE_PATH = '/Users/harshakhiyani/My_Stuff/CODE/GITHUB/Visual-Situmlai-Reconstruction-Using-fMRI-and-Deep-Learning/NSD-Flat Data Proccessing/Caption Embedding/CaptionEmbdedding_77_Tokens.npy'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e25baf-297f-4237-99a0-22af94172706",
   "metadata": {},
   "source": [
    "ClLIPTokenizer turns the words to numbers that the computer can understand.\n",
    "\n",
    "CLIPTextModel turns the numbers to meaningful vectors that have simantic meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07f77e33-620e-4db5-911a-dc462938d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396d1460-5662-4193-b315-618a5b07b2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|██▎                                   | 1699/27767 [01:38<25:10, 17.26it/s]"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- setup as before ---\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "tokenizer = CLIPTokenizer.from_pretrained('openai/clip-vit-large-patch14')\n",
    "model     = CLIPTextModel.from_pretrained('openai/clip-vit-large-patch14')\n",
    "model.eval().to(device)\n",
    "\n",
    "# read all lines\n",
    "with open(CAPTIONS_FILE,'r') as f:\n",
    "    lines = [l.strip() for l in f if l.strip()]\n",
    "\n",
    "features = []\n",
    "\n",
    "# process in chunks of 5 lines\n",
    "for i in tqdm(range(0, len(lines), 5)):\n",
    "    chunk = lines[i:i+5]\n",
    "    prompt = \" \".join(chunk)               # or join with \". \" if you prefer sentence breaks\n",
    "    tokens = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors='pt',\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=77,\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens)\n",
    "        # `last_hidden_state` has shape [1, 77, 768]\n",
    "        # Many SD pipelines use the full sequence, so keep it:\n",
    "        text_embeds = outputs.last_hidden_state.cpu().numpy()\n",
    "        # if you really need a single vector per prompt, you can take the CLS token:\n",
    "        # cls_embed = outputs.last_hidden_state[:, 0].cpu().numpy()\n",
    "\n",
    "    features.append(text_embeds)\n",
    "\n",
    "# stack into array: (num_images, 77, 768)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**tokens)\n",
    "    # last_hidden_state: (1, 77, 512)\n",
    "    embeds = outputs.last_hidden_state.squeeze(0)      # -> (77, 512)\n",
    "    features.append(embeds.cpu().numpy())\n",
    "\n",
    "features = np.stack(features, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a509cf4-4958-4159-bdcf-f9f691e34d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack into array: (num_images, 77, 768)\n",
    "SAVE_PATH = '/Users/harshakhiyani/My_Stuff/CODE/GITHUB/Visual-Situmlai-Reconstruction-Using-fMRI-and-Deep-Learning/Caption Embedding/Caption Embedding_77x768'\n",
    "np.save(SAVE_PATH, features)\n",
    "print(\"Saved embeddings with shape:\", features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe27a52-b21c-4a64-a76d-200d8d6864aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
