{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jatin-Khiyani/Visual-Situmlai-Reconstruction-Using-fMRI-and-Deep-Learning/blob/main/VQ-VAE%20for%20stimuli/VQ_VAE_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84DAr5Cp87v2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LmHKLVjUz1Bd"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMqoHHRv1VHN",
        "outputId": "993317f7-8b49-4e9e-8849-7299844aa6a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Dataset unzipped!\n"
          ]
        }
      ],
      "source": [
        "# ✅ 2. UNZIP FULL DATASET\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = '/content/drive/MyDrive/NSD_Dataset/prepared_nsd_data_subj01.zip'\n",
        "extract_path = '/content/prepared_nsd_data_subj01'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"✅ Dataset unzipped!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgJR8OFt1arq",
        "outputId": "8c62f564-6f2c-4a37-f374-158c99aeb206"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# ✅ 4. INSTALL LIBRARIES\n",
        "!pip install -q diffusers[torch] transformers accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "RWFnHvuE1Xb5",
        "outputId": "42d68fbd-ae5a-4c10-c681-308f848a89cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Cleaned invalid image files\n"
          ]
        }
      ],
      "source": [
        "# ✅ 3. CLEANUP NON-STANDARD IMAGE FILES\n",
        "import os\n",
        "\n",
        "image_dir = '/content/prepared_nsd_data_subj01'\n",
        "files = os.listdir(image_dir)\n",
        "\n",
        "standard = set(f'image_{i:05d}.png' for i in range(195000))\n",
        "\n",
        "for f in files:\n",
        "    if f.startswith('image_') and f.endswith('.png'):\n",
        "        if f not in standard:\n",
        "            print(f\"Deleting non-standard image: {f}\")\n",
        "            os.remove(os.path.join(image_dir, f))\n",
        "\n",
        "print(\"✅ Cleaned invalid image files\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcH4dtcY1dYT"
      },
      "outputs": [],
      "source": [
        "# ✅ 5. EXTRACT VQ-VAE LATENTS IN BATCHES (Optimized with skip logic)\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from diffusers.models import AutoencoderKL\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "image_dir = '/content/prepared_nsd_data_subj01/prepared_nsd_data_subj01'\n",
        "z_save_dir = '/content/drive/MyDrive/NSD_Dataset/z_latents'\n",
        "os.makedirs(z_save_dir, exist_ok=True)\n",
        "\n",
        "# === Load VAE\n",
        "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
        "vae.eval().to(device)\n",
        "\n",
        "# === Preprocess images\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((512, 512)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "# === Load image filenames\n",
        "all_image_files = sorted([\n",
        "    f for f in os.listdir(image_dir)\n",
        "    if f.startswith('image_') and f.endswith('.png') and f[6:-4].isdigit()\n",
        "])\n",
        "\n",
        "# === Skip already processed files\n",
        "already_done = set(f.replace('.pt', '.png') for f in os.listdir(z_save_dir) if f.endswith('.pt'))\n",
        "image_files = [f for f in all_image_files if f not in already_done]\n",
        "\n",
        "batch_size = 36\n",
        "print(f\"Found {len(all_image_files)} total images.\")\n",
        "print(f\"Skipping {len(already_done)} already processed.\")\n",
        "print(f\"Processing {len(image_files)} remaining images in batches of {batch_size}...\")\n",
        "\n",
        "# === Encode in batches\n",
        "with torch.no_grad():\n",
        "    for i in tqdm(range(0, len(image_files), batch_size), desc=\"Encoding with VQ-VAE\"):\n",
        "        batch_files = image_files[i:i+batch_size]\n",
        "        batch_imgs = []\n",
        "\n",
        "        for fname in batch_files:\n",
        "            try:\n",
        "                img = Image.open(os.path.join(image_dir, fname)).convert(\"RGB\")\n",
        "                tensor = preprocess(img)\n",
        "                batch_imgs.append(tensor)\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️ Skipping image {fname} due to error: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not batch_imgs:\n",
        "            continue  # skip empty batch\n",
        "\n",
        "        img_tensor = torch.stack(batch_imgs).to(device)\n",
        "        z = vae.encode(img_tensor).latent_dist.sample() * 0.18215\n",
        "\n",
        "        for j, fname in enumerate(batch_files):\n",
        "            z_path = os.path.join(z_save_dir, fname.replace('.png', '.pt'))\n",
        "            torch.save(z[j].cpu(), z_path)\n",
        "\n",
        "print(\"✅ All latents saved to:\", z_save_dir)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "mount_file_id": "1QUSDNOzvlh0d6jU4WzA-G_Lv-43J0TdT",
      "authorship_tag": "ABX9TyNNNtV7UPLIP63DxoQzIwzq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}