{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jatin-Khiyani/Visual-Situmlai-Reconstruction-Using-fMRI-and-Deep-Learning/blob/main/FastL2LiR/FastL2LiR_copy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35be8fe3-76f6-4fd2-b527-bd313c3bfa85",
      "metadata": {
        "id": "35be8fe3-76f6-4fd2-b527-bd313c3bfa85"
      },
      "source": [
        "# Importing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "wWtaJIYb5JdX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWtaJIYb5JdX",
        "outputId": "eafb4ab8-a836-4799-e9be-f4524d6ae7ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d63ae88c-a9bb-49b9-a6d1-30ed095b781c",
      "metadata": {
        "id": "d63ae88c-a9bb-49b9-a6d1-30ed095b781c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c55769e2-580d-4372-80ee-14fa8d01dd76",
      "metadata": {
        "id": "c55769e2-580d-4372-80ee-14fa8d01dd76"
      },
      "source": [
        "## Importing Captions and fMRI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6d86e039-2955-4c08-aaa0-9aa1b1046b69",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d86e039-2955-4c08-aaa0-9aa1b1046b69",
        "outputId": "dcc8ffdc-201d-4b69-c9a6-c8e6a216ef13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fMRI shape: (27750, 215, 200)\n",
            "CLIP text embeddings shape: (27892, 7680)\n",
            "VQ-VAE latents shape: (27750, 16384)\n"
          ]
        }
      ],
      "source": [
        "fmri = np.load('/content/drive/MyDrive/NSD_Dataset/fmri.npy').astype(np.float32)\n",
        "clip_text = np.load('/content/drive/MyDrive/NSD_Dataset/Caption Embdedding.npy').astype(np.float32)\n",
        "vqvae_latents = np.load('/content/drive/MyDrive/NSD_Dataset/z.npy').astype(np.float32)\n",
        "# === Print shapes ===\n",
        "print(\"fMRI shape:\", fmri.shape)\n",
        "print(\"CLIP text embeddings shape:\", clip_text.shape)\n",
        "print(\"VQ-VAE latents shape:\", vqvae_latents.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b25b3c4-d937-4f58-89d2-9e61cebba1fa",
      "metadata": {
        "id": "4b25b3c4-d937-4f58-89d2-9e61cebba1fa"
      },
      "source": [
        "### Flattening and PreProccessing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q6w8bt-8_cNp",
      "metadata": {
        "id": "Q6w8bt-8_cNp"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "47f9a8f7-ad36-4388-9e89-510558aaef10",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47f9a8f7-ad36-4388-9e89-510558aaef10",
        "outputId": "1fd7832e-3ff1-4d44-cb61-78a3db661dba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fMRI (flattened): (27750, 43000)\n",
            "CLIP text: (27750, 7680)\n",
            "VQ-VAE latents: (27750, 16384)\n"
          ]
        }
      ],
      "source": [
        "# === Flatten fMRI to (N, D_fmri) ===\n",
        "fmri_flat = fmri.reshape(fmri.shape[0], -1)  # shape: (27750, 43000)\n",
        "\n",
        "# === Clip c to match samples ===\n",
        "clip_text = clip_text[:fmri_flat.shape[0]]  # now (27750, 7680)\n",
        "\n",
        "# === Final sanity check ===\n",
        "print(\"fMRI (flattened):\", fmri_flat.shape)\n",
        "print(\"CLIP text:\", clip_text.shape)\n",
        "print(\"VQ-VAE latents:\", vqvae_latents.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1e648da-4abb-44d9-bd8b-a19061f6ab85",
      "metadata": {
        "id": "f1e648da-4abb-44d9-bd8b-a19061f6ab85"
      },
      "source": [
        "## Importing FastL2LiR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler_X = StandardScaler().fit(fmri_flat)\n",
        "X_norm = scaler_X.transform(fmri_flat)\n",
        "\n",
        "scaler_Y = StandardScaler().fit(vqvae_latents)\n",
        "Y_norm = scaler_Y.transform(vqvae_latents)\n"
      ],
      "metadata": {
        "id": "efY8UEgCuHEk"
      },
      "id": "efY8UEgCuHEk",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1b98f5b5-574a-4cb0-8e18-be7b806adf83",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1b98f5b5-574a-4cb0-8e18-be7b806adf83",
        "outputId": "c9486231-0dd7-484d-d52e-74630e45483e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted shape: (27750, 16384)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ─── Ridge Dual Model ────────────────────────────────────────────────────────\n",
        "class RidgeDual:\n",
        "    def __init__(self, alpha=1.0, device=None):\n",
        "        self.alpha = alpha\n",
        "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.W = None\n",
        "        self.b = None\n",
        "\n",
        "    def fit(self, X_np, Y_np):\n",
        "        X = torch.from_numpy(X_np).float().to(self.device)\n",
        "        Y = torch.from_numpy(Y_np).float().to(self.device)\n",
        "\n",
        "        N, D = X.shape\n",
        "\n",
        "        K = X @ X.T  # Kernel matrix\n",
        "        A = torch.linalg.solve(\n",
        "            K + self.alpha * torch.eye(N, device=self.device),\n",
        "            Y\n",
        "        )\n",
        "\n",
        "        self.W = X.T @ A  # (D, T)\n",
        "        self.b = torch.zeros(Y.shape[1], device=self.device)  # No need if using scaled targets\n",
        "\n",
        "    def predict(self, X_np, chunk_size=None):\n",
        "        X = torch.from_numpy(X_np).float().to(self.device)\n",
        "        if chunk_size is None:\n",
        "            return (X @ self.W + self.b).cpu().numpy()\n",
        "\n",
        "        outs = []\n",
        "        for start in tqdm(range(0, X.shape[0], chunk_size), desc=\"Predicting\", unit=\"chunk\"):\n",
        "            chunk = X[start:start+chunk_size]\n",
        "            out = chunk @ self.W + self.b\n",
        "            outs.append(out.cpu().numpy())\n",
        "        return np.vstack(outs)\n",
        "\n",
        "# ─── Train Model ─────────────────────────────────────────────────────────────\n",
        "model = RidgeDual(alpha=1.0)\n",
        "model.fit(X_norm, Y_norm)\n",
        "\n",
        "# ─── Predict (on same normalized input) ──────────────────────────────────────\n",
        "Y_pred_norm = model.predict(X_norm)\n",
        "Y_pred = scaler_Y.inverse_transform(Y_pred_norm)\n",
        "print(\"Predicted shape:\", Y_pred.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "420deae1-df55-45c1-876b-45eda02ba771",
      "metadata": {
        "id": "420deae1-df55-45c1-876b-45eda02ba771"
      },
      "source": [
        "## Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "FeKGDogpUTgD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeKGDogpUTgD",
        "outputId": "16df5454-3b8e-43bc-960f-01108261d854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall MSE: 2.7493e-01\n",
            "Weighted R²: 0.6651\n",
            "Mean sample-wise ρ: 0.8310 ± 0.0414\n",
            "First 5 dims R²: [0.6404062  0.65940213 0.6731678  0.6785784  0.63462466]\n",
            "Median dim R²: 0.6668\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# 1. MSE\n",
        "mse = mean_squared_error(vqvae_latents, Y_pred)\n",
        "print(f\"Overall MSE: {mse:.4e}\")\n",
        "\n",
        "# 2. R² Score\n",
        "r2 = r2_score(vqvae_latents, Y_pred, multioutput='variance_weighted')\n",
        "print(f\"Weighted R²: {r2:.4f}\")\n",
        "\n",
        "# 3. Pearson Correlation (sample-wise)\n",
        "cors = []\n",
        "for i in range(Y_pred.shape[0]):\n",
        "    y_true = vqvae_latents[i]\n",
        "    y_hat = Y_pred[i]\n",
        "    if np.std(y_true) > 0 and np.std(y_hat) > 0:\n",
        "        cors.append(np.corrcoef(y_true, y_hat)[0, 1])\n",
        "cors = np.array(cors)\n",
        "print(f\"Mean sample-wise ρ: {cors.mean():.4f} ± {cors.std():.4f}\")\n",
        "\n",
        "# 4. Per-dimension R²\n",
        "r2_per_dim = r2_score(vqvae_latents, Y_pred, multioutput='raw_values')\n",
        "print(f\"First 5 dims R²: {r2_per_dim[:5]}\")\n",
        "print(f\"Median dim R²: {np.median(r2_per_dim):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UFB4Yx6esMOs"
      },
      "id": "UFB4Yx6esMOs",
      "execution_count": 9,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}